{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "Now, let's train a classification RNN on our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "We start by downloading the data and preparing it for our purposes. This is the exact same thing as was done in the previous notebooks - but it's repeated here for compatibility with Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/mattminder/nlp_intro/refs/heads/main/data/sms_spam_collection/SMSSpamCollection\"\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "data = urllib.request.urlopen(url)\n",
    "\n",
    "# directly load the file from github for compatability with Colab\n",
    "lines_split = [\n",
    "    line.decode().strip().split(\"\\t\")\n",
    "    for line in data\n",
    "]\n",
    "df = pd.DataFrame(lines_split, columns=[\"label\", \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do tokenization and stemming in the same way as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    for letter in '\".,;:!?()_*':\n",
    "        text = text.replace(letter, \" \")  # replace with a space\n",
    "    return text\n",
    "\n",
    "def rudimentary_stemming(word_list):\n",
    "    suffixes_to_remove = [\n",
    "        \"s\",  # plural suffix\n",
    "        \"ing\",\n",
    "        \"ed\",\n",
    "    ]\n",
    "    def remove_suffixes(word):\n",
    "        for suffix in suffixes_to_remove:\n",
    "            word = word.removesuffix(suffix)\n",
    "        return word\n",
    "\n",
    "    return [\n",
    "        remove_suffixes(word)\n",
    "        for word in word_list\n",
    "    ]\n",
    "\n",
    "def preprocessing(text):\n",
    "    lower_case = text.lower()\n",
    "    without_punctuation = remove_punctuation(lower_case)\n",
    "    tokens = without_punctuation.split()  # splits at any whitespace\n",
    "    after_stemming = rudimentary_stemming(tokens)\n",
    "    return after_stemming\n",
    "\n",
    "df[\"word_list\"] = df[\"text\"].apply(preprocessing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we create a dictionary that maps frequent words to a number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequent_word_dictionary(word_list, minimum_count=10):\n",
    "    \"\"\"Create a mapping from frequent words to an integer.\"\"\"\n",
    "    # create a dictionary with the number of occurrences of every word\n",
    "    word_count = pd.Series(word_list).value_counts().to_dict()\n",
    "\n",
    "    # identify the set of words that are frequent enough\n",
    "    relevant_words = {\n",
    "        word for word, count in word_count.items() if count >= minimum_count\n",
    "    }\n",
    "\n",
    "    # turn that set into a dictionary\n",
    "    return {\n",
    "        word: i\n",
    "        for i, word in enumerate(relevant_words)\n",
    "    }\n",
    "\n",
    "\n",
    "frequent_word_dictionary = get_frequent_word_dictionary(\n",
    "    [e for row in df[\"word_list\"].to_list() for e in row]\n",
    ")\n",
    "\n",
    "vocabulary_size = len(frequent_word_dictionary) + 1\n",
    "\n",
    "# this time we assign the last number to unknown words\n",
    "df[\"word_number_list\"] = df[\"word_list\"].apply(\n",
    "    lambda word_list: [\n",
    "        frequent_word_dictionary.get(word, vocabulary_size)\n",
    "        for word in word_list\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to keep texts with more than three words\n",
    "keep = df[\"word_number_list\"].apply(len) > 3\n",
    "df = df[keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Implementation\n",
    "Let's implement our RNN from scratch - note that PyTorch would also already have working implementations.\n",
    "\n",
    "We directly learn a word embedding for every word in our vocabulary - one could (and maybe should) use a pre-trained word embedding here, to leverage transfer learning. \n",
    "\n",
    "**While you don't have to understand all of the code, try to understand the forward method of `RNN` and `RNNClassifier`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class RNN(torch.nn.Module):\n",
    "    \"\"\"RNN module.\"\"\"\n",
    "\n",
    "    def __init__(self, vocabulary_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # part used to go from the word number to the embedding\n",
    "        self.embedder = torch.nn.Embedding(vocabulary_size, hidden_size)\n",
    "        \n",
    "        # part used to process embedding\n",
    "        self.feed_forward = torch.nn.Linear(hidden_size, hidden_size)\n",
    "                                            \n",
    "    def forward(self, words):\n",
    "        # we start with a hidden state of zero\n",
    "        h = torch.zeros((self.hidden_size, ))\n",
    "    \n",
    "        for word in words:\n",
    "            # embed word\n",
    "            word_embedding = self.embedder(word)\n",
    "\n",
    "            # process hidden state\n",
    "            h_processed = self.feed_forward(h)\n",
    "\n",
    "            # add word embedding to processed hidden state and pass through relu\n",
    "            # this is our new hidden state\n",
    "            h = torch.nn.functional.relu(h_processed + word_embedding)\n",
    "\n",
    "        return h\n",
    "    \n",
    "\n",
    "class RNNClassifier(torch.nn.Module):\n",
    "    \"\"\"Classifier, first passing sequence through RNN, then passing through feed-forward classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, vocabulary_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = RNN(vocabulary_size, hidden_size)\n",
    "\n",
    "        # output takes the last hidden state from the RNN and turns it into probabilities \n",
    "        self.output = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 1),\n",
    "            torch.nn.Sigmoid()  # converts into numbers between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, words):\n",
    "        h = self.rnn(words)\n",
    "        return self.output(h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to define how we load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNData(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        words = row[\"word_number_list\"]\n",
    "        is_spam = row[\"label\"] == \"spam\"\n",
    "        return torch.LongTensor(words), torch.FloatTensor([is_spam])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for model training\n",
    "In this next part, we define functions for model training. All of these functions are not specific to RNNs, we could train any kind of classifier with the same logic. It's important to see how we have different epochs, and how we go through every data point in our data once during one epoch. After every epoch, we apply the current state of our neural network to our validation set, to see how good the model currently is.\n",
    "\n",
    "We train the model for a fixed number of iterations. At every iteration, we process every text message by itself, since our custom RNN module does not implement batch support. Here's the function training a given model during one entire epoch (i.e. going once through all of the training examples). We return the average training loss of the epoch to monitor our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def train_single_epoch(train_data, model, optimizer):\n",
    "    \"\"\"Trains the model during single epoch, returns average BCE loss.\"\"\"\n",
    "    bceloss = torch.nn.BCELoss()\n",
    "\n",
    "    train_loss_epoch = []\n",
    "    for words, target in train_data:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(words)\n",
    "        loss = bceloss(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        train_loss_epoch.append(loss.item())\n",
    "        optimizer.step()\n",
    "\n",
    "    return sum(train_loss_epoch) / len(train_loss_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After one epoch of training, we make predictions for the entire test set. Then, we calculate the average loss, precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_single_epoch(test_data, model):\n",
    "    \"\"\"Makes prediction on test set, calculates the validation loss, precision and recall.\"\"\"\n",
    "    bceloss = torch.nn.BCELoss()\n",
    "\n",
    "    true_vals = []\n",
    "    predicted_vals = []\n",
    "    val_loss_epoch = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for words, target in test_data:\n",
    "            output = model(words)\n",
    "            val_loss = bceloss(output, target)\n",
    "            val_loss_epoch.append(val_loss)\n",
    "\n",
    "            # keep these for calculating precision and recall\n",
    "            true_vals.append(target)\n",
    "            predicted_vals.append(output >= .5)\n",
    "\n",
    "    avg_loss = sum(val_loss_epoch) / len(val_loss_epoch)\n",
    "    precision = precision_score(true_vals, predicted_vals)\n",
    "    recall = recall_score(true_vals, predicted_vals)\n",
    "\n",
    "    return avg_loss, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use these two functions in our training function that iterates through the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, train_data, test_data, n_epochs):\n",
    "    \"\"\"Entire training run.\"\"\"\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        print(f\"\\rEpoch {i:4d}/{n_epochs}\", end=\"\")\n",
    "\n",
    "        train_loss = train_single_epoch(train_data, model, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        val_loss, precision, recall = validation_single_epoch(test_data, model)\n",
    "        val_losses.append(val_loss)\n",
    "        val_precision.append(precision)\n",
    "        val_recall.append(recall)\n",
    "\n",
    "    return train_losses, val_losses, val_precision, val_recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "Now, it's finally time to train our model.\n",
    "Split into training and validation data exactly like before for comparability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=.2, random_state=123)\n",
    "\n",
    "train_data = RNNData(train)\n",
    "test_data = RNNData(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our model to use an embedding size of 5, during 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "hidden_size = 5\n",
    "learning_rate = .001\n",
    "\n",
    "model = RNNClassifier(vocabulary_size + 1, hidden_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses, val_losses, val_precision, val_recall = train_model(model, optimizer, train_data, test_data, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training and validation loss, as it evolves during training. **Do we observe overfitting?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.legend([\"train loss\", \"test loss\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at precision and recall curves throughout training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(val_precision)\n",
    "plt.plot(val_recall)\n",
    "\n",
    "plt.legend([\"precision\", \"recall\"])\n",
    "plt.xlabel(\"Number of Epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that precision and recall improve quickly throughout the first couple of iterations, then quickly stabilize. Here are the metrics at the end of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision\", val_precision[-1])\n",
    "print(\"Recall\", val_recall[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performance varies from training run to training run, since the network initialization is random. Overall, the performance is worse than the performance achieved in our simple \"bag of words\" model. Why could that be the case? What does it say about our task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a smaller model\n",
    "Since we're seeing overfitting, maybe we should use a model with less hidden states (this makes our model less flexible, and thus less prone to overfitting). We can train a model with just two hidden states as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "hidden_size = 2\n",
    "learning_rate = .001\n",
    "\n",
    "model = RNNClassifier(vocabulary_size + 1, hidden_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses, val_losses, val_precision, val_recall = train_model(model, optimizer, train_data, test_data, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.legend([\"train loss\", \"test loss\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(val_precision)\n",
    "plt.plot(val_recall)\n",
    "\n",
    "plt.legend([\"precision\", \"recall\"])\n",
    "plt.xlabel(\"Number of Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision\", val_precision[-1])\n",
    "print(\"Recall\", val_recall[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still see some overfitting, and our performance is still a lot worse than bag of words. This suggests a couple of things:\n",
    "- Maybe word order isn't important for spam detection.\n",
    "- We don't have enough data to train big neural networks - we have to show the same data a lot of times, which leads to overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
