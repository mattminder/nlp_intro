{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing\n",
    "\n",
    "Within these notebooks, we will explore difficulties and particularities when working with text data. As an example task, we make use of the SMS Spam Collection. It contains roughly 5'600 messages, that have manually been classified into \"spam\" or \"ham\" (non-spam).\n",
    "\n",
    "Trigger warning: Some of the text messages contain swear words or sexual content.\n",
    "\n",
    "## Data Loading\n",
    "\n",
    "Let's load the data and have a first look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/mattminder/nlp_intro/refs/heads/main/data/sms_spam_collection/SMSSpamCollection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "data = urllib.request.urlopen(url)\n",
    "\n",
    "# directly load the file from github for compatability with Colab\n",
    "lines_split = [\n",
    "    line.decode().strip().split(\"\\t\")\n",
    "    for line in data\n",
    "]\n",
    "df = pd.DataFrame(lines_split, columns=[\"label\", \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at five random messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with text messages, our data is quite messy. \n",
    "\n",
    "What's the ratio of ham to spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features from Text\n",
    "We cannot feed text data into a model - we need numerical values instead. In our very first model, we won't look at the word contents, but instead at other features that we can extract from text. In particular, we will calculate:\n",
    "- The length of a text message\n",
    "- The number of punctuation that was used\n",
    "- The number of upper-case letters\n",
    "- The number of numbers\n",
    "- The number of occurrances of the letter X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_features = df.copy()\n",
    "\n",
    "simple_features[\"length\"] = df[\"text\"].apply(len)\n",
    "simple_features[\"number_punctuation\"] = df[\"text\"].apply(lambda x: sum(1 for letter in x if letter in '\".,;:!?()_*'))\n",
    "simple_features[\"number_uppercase\"] = df[\"text\"].apply(lambda x: sum(1 for letter in x if letter!=letter.lower()))\n",
    "simple_features[\"number_numbers\"] = df[\"text\"].apply(lambda x: sum(1 for letter in x if letter in \"0123456789\"))\n",
    "simple_features[\"number_x\"] = df[\"text\"].apply(lambda x: sum(1 for letter in x if letter in \"xX\"))\n",
    "\n",
    "simple_features[\"is_spam\"] = df[\"label\"] == \"spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the effect of our new features as box-plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(12, 4))\n",
    "sns.violinplot(simple_features, x=\"is_spam\", y=\"length\", ax=axs[0])\n",
    "sns.violinplot(simple_features, x=\"is_spam\", y=\"number_punctuation\", ax=axs[1])\n",
    "sns.violinplot(simple_features, x=\"is_spam\", y=\"number_uppercase\", ax=axs[2])\n",
    "sns.violinplot(simple_features, x=\"is_spam\", y=\"number_numbers\", ax=axs[3])\n",
    "sns.violinplot(simple_features, x=\"is_spam\", y=\"number_x\", ax=axs[4])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well do we perform with a simple logistic model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "train, test = train_test_split(simple_features, test_size=.2, random_state=123)\n",
    "\n",
    "features = [\n",
    "    \"length\",\n",
    "    \"number_punctuation\",\n",
    "    \"number_uppercase\",\n",
    "    \"number_numbers\",\n",
    "    \"number_x\",\n",
    "]\n",
    "\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(train[features], train[\"is_spam\"])\n",
    "\n",
    "test_predictions = logistic_regression.predict(test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate precision and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision:\", precision_score(test[\"is_spam\"], test_predictions))\n",
    "print(\"Recall:\", recall_score(test[\"is_spam\"], test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we already get very good performance without even looking at the words in the data.\n",
    "\n",
    "## Converting text into numerical values\n",
    "Next, we want to create models that actually look at the content of our messages. To do this, we have to convert the content of our messages to some numerical representation, that we can then pass it onto a model.\n",
    "\n",
    "Let's look at a single text message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = df.loc[4233, \"text\"]\n",
    "example_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we do that? Since our language is composed of words, it seems intuitive to split our messages into words. Then, we can assign a unique number to every word. Splitting a text document into smaller parts (typically words or parts of words) is called **tokenization**.\n",
    "\n",
    "To split our messages into words, we first remove all punctuation except ', and then split at every white-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    for letter in '\".,;:!?()_*':\n",
    "        text = text.replace(letter, \" \")  # replace with a space\n",
    "    return text\n",
    "\n",
    "def to_word_list(text):\n",
    "    without_punctuation = remove_punctuation(text)\n",
    "    return without_punctuation.split()  # splits at any whitespace\n",
    "\n",
    "word_list = to_word_list(example_text)\n",
    "word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we give an unique id to every word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_dictionary(word_list):\n",
    "    \"\"\"Create a mapping from every word to an integer.\"\"\"\n",
    "    return {\n",
    "        word: i\n",
    "        for i, word in enumerate(set(word_list))\n",
    "    }\n",
    "\n",
    "\n",
    "word_dict = get_word_dictionary(word_list)\n",
    "number_list = [word_dict[word] for word in word_list]\n",
    "\n",
    "# let's look at the word dictionary\n",
    "word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is now encoded as the following list of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have obtained a first numerical representation of our sample data. However, passing it like this to some model doesn't make much sense: The order of the words is chosen arbitrarily, so the numbers don't really mean anything. This is a problem: The model will consider numbers that are closer together to be more similar than numbers that are far apart.\n",
    "\n",
    "In order to make every word equally far apart, we can turn our sentence into a so-called one-hot encoding. We map every word to a vector, where the vector size is equal to the number of words in the corpus. This vector has value zero everywhere except for one row: We put the value 1 into the row corresponding to the number that our word was given.\n",
    "\n",
    "For example: If we have 5 words in total, we would create the following vectors: \n",
    "- Word 0: `[1, 0, 0, 0, 0]`\n",
    "- Word 1: `[0, 1, 0, 0, 0]`\n",
    "- Word 2: `[0, 0, 1, 0, 0]`\n",
    "- Word 3: `[0, 0, 0, 1, 0]`\n",
    "- Word 4: `[0, 0, 0, 0, 1]`\n",
    "\n",
    "Let's implement this in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_one_hot(number, vocabulary_size):\n",
    "    \"\"\"Function that takes a single integer and the vocabulary size, and creates a one-hot vector.\"\"\"\n",
    "    output = [0] * vocabulary_size\n",
    "\n",
    "    # negative numbers will be reserved for when we encounter a new word\n",
    "    if number >= 0:\n",
    "        output[number] = 1\n",
    "    return output\n",
    "\n",
    "number_to_one_hot(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can encode the entire sentence\n",
    "def number_list_to_one_hot(number_list, vocabulary_size):\n",
    "    return [\n",
    "        number_to_one_hot(number, vocabulary_size)\n",
    "        for number in number_list\n",
    "    ]\n",
    "\n",
    "vocabulary_size = max(number_list) + 1\n",
    "\n",
    "sentence_one_hot = number_list_to_one_hot(number_list, vocabulary_size)\n",
    "sentence_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have converted our text into a numerical representation, where the distance between each word is equal.\n",
    "\n",
    "## Encoding all text-messages\n",
    "\n",
    "Of course we don't want to encode only one message, but all text messages at once. We can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"word_lists\"] = df[\"text\"].apply(to_word_list)\n",
    "\n",
    "def flatten_list_of_lists(list_of_lists):\n",
    "    \"\"\"Flattens the list of lists [[a], [b, c]] to [a, b, c].\"\"\"\n",
    "    return [\n",
    "        e\n",
    "        for sublist in list_of_lists\n",
    "        for e in sublist\n",
    "    ]\n",
    "\n",
    "# create a list of all words\n",
    "list_of_all_words = flatten_list_of_lists(df[\"word_lists\"].to_list())\n",
    "\n",
    "# create the dictionary\n",
    "full_dict = get_word_dictionary(list_of_all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How big is our dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have encountered over 11'000 different words in our 5'600 messages alone. Let's look at some of the words that we've found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pd.DataFrame(full_dict.items(), columns=[\"word\", \"index\"])\n",
    "    .sort_values(\"word\")\n",
    "    .sample(10, random_state=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we observe?\n",
    "- Some words are proper names.\n",
    "- Some words aren't really words, but just onomatopeas.\n",
    "- Some words are grammatical variations of a single word stem.\n",
    "- Some words are misspellings. \n",
    "- Some words are upper-case.\n",
    "- Some words are in another language.\n",
    "\n",
    "How could we improve this?\n",
    "\n",
    "1. We could remove words that are rare. This will remove misspelled words, but also rare names and places.\n",
    "2. We could convert every word to its grammatical origin (for example: remove the plural, or conjugate to the base word). This is called \"stemming\". We would reduce the vocabulary size drastically, but also lose important information (plural or not, etc).\n",
    "3. We could convert every word to lower-case, since upper- and lower-case doesn't change the meaning of the word in English.\n",
    "\n",
    "Or, alternatively:\n",
    "\n",
    "4. We could split rare and conjugated words further: For example, split killing into [kill, #ing], where # is a special character to denote that we split a word. This is more complex to handle, but can still handle rare words and conjugation. This last approach is what is typically done in modern large language models. You can find a demo under this link: https://codesandbox.io/s/gpt-tokenizer-tjcjoz.\n",
    "\n",
    "\n",
    "## Stemming and removing rare words\n",
    "\n",
    "Let's implement the first three points mentioned above. We do very rudimentary stemming by removing certain suffixes, and only keep words that occur more than 10 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower_case(word_list):\n",
    "    return [\n",
    "        word.lower() for word in word_list\n",
    "    ]\n",
    "\n",
    "to_lower_case(df.loc[4233, \"word_lists\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rudimentary_stemming(word_list):\n",
    "    suffixes_to_remove = [\n",
    "        \"s\",  # plural suffix\n",
    "        \"ing\",\n",
    "        \"ed\",\n",
    "    ]\n",
    "    def remove_suffixes(word):\n",
    "        for suffix in suffixes_to_remove:\n",
    "            word = word.removesuffix(suffix)\n",
    "        return word\n",
    "\n",
    "    return [\n",
    "        remove_suffixes(word)\n",
    "        for word in word_list\n",
    "    ]\n",
    "\n",
    "to_show_index = 20\n",
    "\n",
    "print(\"Original Sentence:  \", df.loc[to_show_index, \"text\"])\n",
    "print(\"Stemmed Version:    \", rudimentary_stemming(df.loc[20, \"word_lists\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our rudimentary stemming has a lot of flaws. There are more sophisticated algorithms in practice, for example the [Porter Stemming Algorithm](https://de.wikipedia.org/wiki/Porter-Stemmer-Algorithmus).\n",
    "\n",
    "Now we can create a new column with the cleaned word lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_word_lists\"] = df[\"word_lists\"].apply(\n",
    "    lambda x: rudimentary_stemming(to_lower_case(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our dictionary next - now with the criterion, that every word needs to appear at least 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequent_word_dictionary(word_list, minimum_count=10):\n",
    "    \"\"\"Create a mapping from frequent words to an integer.\"\"\"\n",
    "    # create a dictionary with the number of occurrences of every word\n",
    "    word_count = pd.Series(word_list).value_counts().to_dict()\n",
    "\n",
    "    # identify the set of words that are frequent enough\n",
    "    relevant_words = {\n",
    "        word for word, count in word_count.items() if count >= minimum_count\n",
    "    }\n",
    "\n",
    "    # turn that set into a dictionary\n",
    "    return {\n",
    "        word: i\n",
    "        for i, word in enumerate(relevant_words)\n",
    "    }\n",
    "\n",
    "\n",
    "# create a list of all clean words\n",
    "list_of_clean_words = flatten_list_of_lists(df[\"cleaned_word_lists\"].to_list())\n",
    "\n",
    "# create the dictionary\n",
    "frequent_dict = get_frequent_word_dictionary(list_of_clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many different words do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frequent_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems more sensible for a text corpus with 5'600 text messages.\n",
    "\n",
    "We then apply the one-hot encoder to all of our text messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df[\"word_number_list\"] = df[\"cleaned_word_lists\"].apply(\n",
    "    lambda word_list: [\n",
    "        frequent_dict.get(word, -1)\n",
    "        for word in word_list \n",
    "    ]\n",
    ")\n",
    "\n",
    "df[\"one_hot_word_encoding\"] = df[\"word_number_list\"].apply(\n",
    "    lambda number_list: np.array(\n",
    "        number_list_to_one_hot(number_list, vocabulary_size=len(frequent_dict) + 1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 0\n",
    "print(\"One-Hot Encoding:\\n\", df[\"one_hot_word_encoding\"][ix])\n",
    "print(\"Shape:\", df[\"one_hot_word_encoding\"][ix].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-hot encoding has shape equals to the number of words times the total size of the dictionary.\n",
    "\n",
    "Let's look at another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 1\n",
    "print(\"One-Hot Encoding:\\n\", df[\"one_hot_word_encoding\"][ix])\n",
    "print(\"Shape:\", df[\"one_hot_word_encoding\"][ix].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this second text message was shorter, the encoding grew shorter as well.\n",
    "\n",
    "This is a problem: Standard models such as linear regressions, but also feed-forward neural networks, want our input always to have the same size. How can we deal with the different lenghts of our inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "The easiest way to deal with varying input lengths is to calculate the sum of the one-hot encodings of every word. Conceptually, this just calculates how many times a word occurs in the text. This is why this method is called **bag of words**.\n",
    "\n",
    "For example: The bag of words-encoding of the sentence \"hello, how are you and how are your parents?\" is:\n",
    "\n",
    "|I|you|hello|how|are|parents|your|and|\n",
    "|-|---|-----|---|---|-------|----|---|\n",
    "|0|1  |1    |2  |2  |1      |1   |1  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the bag of words for all of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bag_of_words\"] = df[\"one_hot_word_encoding\"].apply(lambda x: np.sum(x, axis=0))\n",
    "\n",
    "df[\"bag_of_words\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This we can now use to train our model on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_df = df[\"bag_of_words\"].apply(pd.Series)\n",
    "target = df[\"label\"] == \"spam\"\n",
    "\n",
    "# we have to use the same test_size and random state as above to ensure comparability\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    bag_of_words_df, target, test_size=.2, random_state=123\n",
    ")\n",
    "\n",
    "# let's look at our training data\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a linear model on that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model = LogisticRegression()\n",
    "bow_model.fit(train_x.fillna(0), train_y)\n",
    "\n",
    "bow_predictions = bow_model.predict(test_x.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision:\", precision_score(test[\"is_spam\"], bow_predictions))\n",
    "print(\"Recall:\", recall_score(test[\"is_spam\"], bow_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that by looking at our content, we could greatly improve the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "Our bag of words model has room for improvement:\n",
    "- Our vectors are very sparse and don't contain a lot of information\n",
    "- We don't take into account the order of the words\n",
    "- We don't distinguish between lower- and upper-case words, punctuation, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
