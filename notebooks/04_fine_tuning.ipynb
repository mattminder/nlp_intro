{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Foundation Model\n",
    "\n",
    "In this example, we show how to use foundation models for fine-tuning to our spam detection task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/mattminder/nlp_intro/refs/heads/main/data/sms_spam_collection/SMSSpamCollection\"\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "data = urllib.request.urlopen(url)\n",
    "\n",
    "# directly load the file from github for compatability with Colab\n",
    "lines_split = [\n",
    "    line.decode().strip().split(\"\\t\")\n",
    "    for line in data\n",
    "]\n",
    "df = pd.DataFrame(lines_split, columns=[\"label\", \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Pre-Trained Model\n",
    "We will use the transformers library from huggingface for fine-tuning. The following cell will take some time to execute, because the model has to be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"google-bert/bert-base-cased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use another model from [huggingface](https://huggingface.co/). Reasons for why we chose this model:\n",
    "- BERT models were trained on a \"fill-in-the-gap\" task, which makes them better for classification than models trained on a \"predict the next word\" task.\n",
    "- We use a cased model (which means that it distinguishes between capital and small letters), since this may be useful for predicting spam.\n",
    "- The BERT model is a _classic_ - but note that many better models have been trained since.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "For splitting the texts into tokens, we have to use the same tokenizer as was used when training the model. We download it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what happens when we tokenize text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = df[\"text\"].iloc[0]\n",
    "print(\"Text:\", some_text)\n",
    "print(\"Token IDs:\", tokenizer(some_text)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like our word dictionary, our tokenizer turns a text into a sequence of IDs that can be fed to a network.\n",
    "\n",
    "We tokenize all our datapoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] = df[\"text\"].apply(lambda x: tokenizer(x, padding=\"max_length\", max_length=64, truncation=True)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did in addition to before is to make every embedded text equally long - and just retain the 64 first tokens. It should become clear after 64 tokens whether something is spam or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our Data\n",
    "We split our data into training and test set as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our own dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Data(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.x = torch.LongTensor(df[\"tokens\"].tolist())\n",
    "        self.y = torch.LongTensor((df[\"label\"] == \"spam\").tolist())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __getitems__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    print(data)\n",
    "    tensors, targets = data\n",
    "    targets = torch.stack(targets)\n",
    "    return torch.stack(tensors), targets\n",
    "\n",
    "# Create data and loader objects\n",
    "train_data = Data(train)\n",
    "test_data = Data(test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=lambda x: x)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=256, shuffle=False, collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "Finetuning works in the exact same way as when we train a model from scratch - except that we already start from trained weights, and not from random initialization. This means that we need a lot less epochs - a single epoch should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine the model here, to make sure we \"restart\" fine-tuning\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "n_epochs = 1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch + 1} of {n_epochs}\")\n",
    "    for x, y in train_loader:\n",
    "        pred = model(x, attention_mask=(x!=0))\n",
    "        loss = ce(pred.logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.detach().item())\n",
    "        print(f\"\\rLoss {losses[-1]:2.2e}. Iteration {len(losses)} of {len(train_loader)}.\", end=\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We iterate through the entire test set to create predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    for i, (x, _) in enumerate(test_loader):\n",
    "        print(f\"\\r{i:3d} of {len(test_loader)}\", end=\"\")\n",
    "        pred = model(x, attention_mask=(x > 0))\n",
    "        predictions.append(pred)\n",
    "\n",
    "# get predicted labels\n",
    "predictions_tensor = torch.concat([p.logits for p in predictions])\n",
    "predicted_label = predictions_tensor[:, 1] >= predictions_tensor[:, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we measure the quality with precision and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(\"Precision:\", precision_score(test[\"label\"] == \"spam\", predicted_label))\n",
    "print(\"Recall:\", recall_score(test[\"label\"] == \"spam\", predicted_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that with a single pass through the data, our model performs very well. This shows the power of fine-tuning. However, we also see that applying our model is quite slow. For the spam detection use-case, it is probably too slow - if we want to detect spam in thousands of mails per second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
