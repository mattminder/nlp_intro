{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "We saw in our previous notebook that the one-hot vectors and the bag-of-word-vectors were very sparse (i.e. they had a lot of zeros), and thus didn't contain much information. In particular, every word has the same distance to all other words in one-hot encodings. We would gain a lot if we are able to encode our words such that similar words are close together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "We start by downloading the data and preparing it for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/mattminder/nlp_intro/refs/heads/main/data/sms_spam_collection/SMSSpamCollection\"\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "data = urllib.request.urlopen(url)\n",
    "\n",
    "# directly load the file from github for compatability with Colab\n",
    "lines_split = [\n",
    "    line.decode().strip().split(\"\\t\")\n",
    "    for line in data\n",
    "]\n",
    "df = pd.DataFrame(lines_split, columns=[\"label\", \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do tokenization and stemming in the same way as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    for letter in '\".,;:!?()_*':\n",
    "        text = text.replace(letter, \" \")  # replace with a space\n",
    "    return text\n",
    "\n",
    "def rudimentary_stemming(word_list):\n",
    "    suffixes_to_remove = [\n",
    "        \"s\",  # plural suffix\n",
    "        \"ing\",\n",
    "        \"ed\",\n",
    "    ]\n",
    "    def remove_suffixes(word):\n",
    "        for suffix in suffixes_to_remove:\n",
    "            word = word.removesuffix(suffix)\n",
    "        return word\n",
    "\n",
    "    return [\n",
    "        remove_suffixes(word)\n",
    "        for word in word_list\n",
    "    ]\n",
    "\n",
    "def preprocessing(text):\n",
    "    lower_case = text.lower()\n",
    "    without_punctuation = remove_punctuation(lower_case)\n",
    "    tokens = without_punctuation.split()  # splits at any whitespace\n",
    "    after_stemming = rudimentary_stemming(tokens)\n",
    "    return after_stemming\n",
    "\n",
    "df[\"word_list\"] = df[\"text\"].apply(preprocessing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we create a dictionary that maps frequent words to a number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequent_word_dictionary(word_list, minimum_count=10):\n",
    "    \"\"\"Create a mapping from frequent words to an integer.\"\"\"\n",
    "    # create a dictionary with the number of occurrences of every word\n",
    "    word_count = pd.Series(word_list).value_counts().to_dict()\n",
    "\n",
    "    # identify the set of words that are frequent enough\n",
    "    relevant_words = {\n",
    "        word for word, count in word_count.items() if count >= minimum_count\n",
    "    }\n",
    "\n",
    "    # turn that set into a dictionary\n",
    "    return {\n",
    "        word: i\n",
    "        for i, word in enumerate(relevant_words)\n",
    "    }\n",
    "\n",
    "\n",
    "frequent_word_dictionary = get_frequent_word_dictionary(\n",
    "    [e for row in df[\"word_list\"].to_list() for e in row]\n",
    ")\n",
    "\n",
    "vocabulary_size = len(frequent_word_dictionary) + 1\n",
    "\n",
    "# this time we assign the last number to unknown words\n",
    "df[\"word_number_list\"] = df[\"word_list\"].apply(\n",
    "    lambda word_list: [\n",
    "        frequent_word_dictionary.get(word, vocabulary_size)\n",
    "        for word in word_list\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to keep texts with more than three words\n",
    "keep = df[\"word_number_list\"].apply(len) > 3\n",
    "df = df[keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram Model\n",
    "One way to calculate such an encoding is to use a skip-gram model. It takes a word and wants to predict all words surrounding the input word in a given sentence.\n",
    "\n",
    "For example in the sentence:\n",
    "\"I go out to *take* *the* **dog** *for* *a* walk\".\n",
    "\n",
    "If we provide the word **dog** as an input, we would want to correctly predict all the words marked in italic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SkipGram(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # We randomly initialize two matrices, embedding and context\n",
    "        self.embedding_matrix = torch.nn.Parameter(\n",
    "            torch.randn(size=(vocabulary_size, embedding_dim))\n",
    "        )\n",
    "        self.context_matrix = torch.nn.Parameter(\n",
    "            torch.randn(size=(embedding_dim, vocabulary_size))\n",
    "        )\n",
    "\n",
    "        # in the end, we calculate Softmax according to the last axis\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = x @ self.embedding_matrix\n",
    "        return emb @ self.context_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to define how we load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import functools\n",
    "\n",
    "class SkipGramData(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, vocabulary_size):\n",
    "        self.df = df\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "    def _number_to_one_hot(self, number):\n",
    "        return torch.nn.functional.one_hot(torch.Tensor([number]).long(), self.vocabulary_size).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row_ix = idx % len(self.df)\n",
    "        word_number_list = self.df[\"word_number_list\"].iloc[row_ix]\n",
    "\n",
    "        # we randomly choose an input and target word, at most 2 words apart\n",
    "        word_number = random.sample(range(len(word_number_list)), 1)[0]\n",
    "\n",
    "        query = word_number_list[word_number]\n",
    "\n",
    "        targets = [\n",
    "            self._number_to_one_hot(word_number_list[word_number + delta])\n",
    "            for delta in [-2, -1, 1, 2]\n",
    "            if word_number + delta >= 0 and word_number + delta < len(word_number_list)\n",
    "        ]\n",
    "        target = functools.reduce(lambda x, y: x + y, targets)\n",
    "        target = torch.clip(target, 0, 1)\n",
    "\n",
    "        return (\n",
    "            self._number_to_one_hot(query),\n",
    "            target,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model and the data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_gram_train_data = SkipGramData(train, vocabulary_size + 1)\n",
    "skip_gram_test_data = SkipGramData(test, vocabulary_size + 1)\n",
    "\n",
    "def my_collate(batch):\n",
    "    queries, targets = zip(*batch)\n",
    "    return torch.concat(queries), torch.concat(targets)\n",
    "\n",
    "# load data in batches\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    skip_gram_train_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=my_collate\n",
    ")\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    skip_gram_test_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=my_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we embed into two dimensions only\n",
    "skip_gram_model = SkipGram(vocabulary_size + 1, 2)\n",
    "optimizer = torch.optim.AdamW(skip_gram_model.parameters(), lr=.001)\n",
    "\n",
    "n_epochs = 200\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # train during a single epoch\n",
    "    train_loss_epoch = []\n",
    "    for queries, targets in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = skip_gram_model(queries)\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "        loss.backward()\n",
    "        train_loss_epoch.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "    # validation run\n",
    "    val_loss_epoch = []\n",
    "    with torch.no_grad():\n",
    "        for queries, targets in train_data_loader:\n",
    "            outputs = skip_gram_model(queries)\n",
    "            val_loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "            val_loss_epoch.append(val_loss)\n",
    "\n",
    "    train_losses.append(sum(train_loss_epoch) / len(train_loss_epoch))\n",
    "    val_losses.append(sum(val_loss_epoch) / len(val_loss_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "\n",
    "plt.legend([\"train loss\", \"test loss\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings\n",
    "Now, we can have a look at the embeddings that the model has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.DataFrame(skip_gram_model.embedding_matrix.data)\n",
    "embeddings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct the inverse dictionary, to know what the entries correspond to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_dict = {v: k for k, v in frequent_word_dictionary.items()}\n",
    "embeddings.index = [inverse_dict.get(i, \"__unknown\") for i in range(len(embeddings))]\n",
    "embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.scatter(embeddings[0], embeddings[1])\n",
    "for word, row in embeddings.iterrows():\n",
    "    plt.text(row[0], row[1], word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the embeddings did not learn a lot of useful signal. The only thing that is consistently close to each other are the words \"sorry\", \"i'll\", \"call\", and \"later\".\n",
    "\n",
    "This was somewhat to be expected:\n",
    "- With 5'600 messages, our corpus is very small for natural language processing.\n",
    "- Messages are very short, so we only have few words per document.\n",
    "- Text quality is poor, since there are many typos, slang and abbreviation.\n",
    "\n",
    "All of this means that we don't have many example usages for all of the words in our corpus. We therefore don't see them in enough situations to properly learn what context they are used in. The only exception is the combination sorry, call and later which is sufficiently abundant for our model to learn that these words are often used together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Embeddings\n",
    "Let's see if using a bigger embedding size can improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 20\n",
    "\n",
    "skip_gram_model = SkipGram(vocabulary_size + 1, embedding_size)\n",
    "optimizer = torch.optim.AdamW(skip_gram_model.parameters())\n",
    "\n",
    "n_epochs = 200\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # train during a single epoch\n",
    "    train_loss_epoch = []\n",
    "    for queries, targets in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = skip_gram_model(queries)\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "        loss.backward()\n",
    "        train_loss_epoch.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "    # validation run\n",
    "    val_loss_epoch = []\n",
    "    with torch.no_grad():\n",
    "        for queries, targets in train_data_loader:\n",
    "            outputs = skip_gram_model(queries)\n",
    "            val_loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "            val_loss_epoch.append(val_loss)\n",
    "\n",
    "    train_losses.append(sum(train_loss_epoch) / len(train_loss_epoch))\n",
    "    val_losses.append(sum(val_loss_epoch) / len(val_loss_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "\n",
    "plt.legend([\"train loss\", \"test loss\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the larger model slightly improved the loss. However, it is not until we use the embedding on a downstream task that we see if our embedding is any good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using these embeddings to detect spam\n",
    "\n",
    "To see whether our embedding is useful, we use them as an input to our spam classification. We will use a small neural network for the classifier, because we do not expect that we can separate the messages linearly.\n",
    "\n",
    "As was the case with bag of words, we again face the challenge of somehow having to aggregate the embeddings of sentences with different length to always create a vector of the same size. We do this by calculating for every sentence the average value of the embedding. This is done in the code below (note that this isn't the most efficient way to do this, but hopefully the clearest to read)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.DataFrame(skip_gram_model.embedding_matrix.data)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mean_embedding = []\n",
    "\n",
    "# Iterate through every sentence\n",
    "for word_number_list in df[\"word_number_list\"]:\n",
    "    # Calculate the mean embedding for every word in that sentence\n",
    "    embedding = np.zeros((embeddings.shape[1]), dtype=float)\n",
    "    count = 0\n",
    "    for word_number in word_number_list:\n",
    "        if word_number in embeddings.index:\n",
    "            embedding = embedding + embeddings.loc[word_number]\n",
    "            count = count + 1\n",
    "    \n",
    "    if count == 0:\n",
    "        mean_embedding.append(embedding)\n",
    "    else:\n",
    "        mean_embedding.append(embedding / count)\n",
    "\n",
    "# Turn everything into an array\n",
    "mean_embedding_df = pd.DataFrame(mean_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the data for training, by using the same train-test-split as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "target = df[\"label\"] == \"spam\"\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    mean_embedding_df, target, test_size=.2, random_state=123\n",
    ")\n",
    "\n",
    "# Convert the matrices to PyTorch\n",
    "train_x_tensor = torch.tensor(train_x.values).float()\n",
    "test_x_tensor = torch.tensor(test_x.values).float()\n",
    "train_y_tensor = torch.tensor(train_y.values).float().unsqueeze(-1)\n",
    "test_y_tensor = torch.tensor(test_y.values).float().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model and the optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dimensions, hidden_dimensions, number_hidden):\n",
    "    \"\"\"Creates a feed-forward neural network with ReLU activations.\"\"\"\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(input_dimensions, hidden_dimensions),\n",
    "        torch.nn.ReLU(),\n",
    "        *[\n",
    "            layer\n",
    "            for _ in range(number_hidden)\n",
    "            for layer in (torch.nn.Linear(hidden_dimensions, hidden_dimensions), torch.nn.ReLU())\n",
    "        ],\n",
    "        torch.nn.Linear(hidden_dimensions, 1)\n",
    "    )\n",
    "\n",
    "model = create_model(embedding_size, hidden_dimensions=20, number_hidden=5)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "learning_curve = []\n",
    "\n",
    "for epoch in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(train_x_tensor)\n",
    "    loss = ce_loss(output, train_y_tensor)\n",
    "    loss.backward()\n",
    "    \n",
    "    train_loss_epoch.append(loss.item())\n",
    "    optimizer.step()\n",
    "    learning_curve.append(loss.detach().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the learning curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learning_curve);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predictions on the test set\n",
    "predictions = (model(test_x_tensor) > 0).numpy()\n",
    "\n",
    "print(\"Precision:\", precision_score(test_y, predictions))\n",
    "print(\"Recall:\", recall_score(test_y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the performance is quite a bit lower than what we obtain from the bag-of-words-model. However, note that we achieved this performance by reducing the 1'000 dimensions of bag-of-words to just 20. By using a larger embedding size, we would expect our model to perform a bit better.\n",
    "\n",
    "Moreover, as we have seen, the little amount of text that we have is not sufficient to learn a rich embedding from. We would expect a higher performance if we used embeddings trained on more text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
